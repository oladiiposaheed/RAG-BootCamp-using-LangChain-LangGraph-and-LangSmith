{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12abe1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26066e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LangChain imports\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# Vector store + embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5f1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare documents#\n",
    "\n",
    "docs = [\n",
    "    'Green tea contains antioxidants that improve health.',\n",
    "    'Black tea is rich in caffeine and boosts energy.',\n",
    "    'Catechins in green tea help reduce cholesterol.',\n",
    "    'Tea ceremonies are an important cultural tradition in Japan.',\n",
    "    'Green tea may improve metabolism and aid in weight loss.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9ad728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8232\\3096126841.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "# Build Vector Store\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb895f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Retriever (MMR)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr', # Enable MMR reranking\n",
    "    search_kwargs={\n",
    "        'k':3, \n",
    "        'fetch_k': 10, \n",
    "        'lambda_mult': 0.7\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a32d03",
   "metadata": {},
   "source": [
    "Prompt for Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a94b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answer = PromptTemplate.from_template(\n",
    "    '''\n",
    "    Use the following context to answer the question:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f34f0",
   "metadata": {},
   "source": [
    "Prompt for Document Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e9dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rank = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. Your task is to rank the following documents \n",
    "    from most to least relevant to the user's question.\n",
    "    \n",
    "    User Question: '{question}'\n",
    "    \n",
    "    Documents:\n",
    "    {documents}\n",
    "    \n",
    "    Instructions:\n",
    "    - Think carefully about the relevance of each document to the user's question.\n",
    "    - Return a list of document indices in ranked order, starting from the most relevant.\n",
    "    \n",
    "    Output format:\n",
    "    Comma-separated document indices (e.g., 2, 3, 0, ...)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e9e53",
   "metadata": {},
   "source": [
    "LLM + Parser Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a29f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "\n",
    "# Answer generation chain\n",
    "rag_chain_answer = prompt_answer | llm | StrOutputParser()\n",
    "\n",
    "# Document ranking chain\n",
    "rag_chain_rank = prompt_rank | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71423d61",
   "metadata": {},
   "source": [
    "Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb049485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8232\\1808091942.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: Green tea contains antioxidants that improve health and may improve metabolism and aid in weight loss.\n",
      "\n",
      "\n",
      "Ranked Document Indices: 0, 1, 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'What are the health benefits of green tea?'\n",
    "\n",
    "# Retrieve docs with MMR\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Prepare context for answer chain\n",
    "context = '\\n'.join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Prepare documents for ranking chain\n",
    "documents = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "# Run answer generation\n",
    "answer = rag_chain_answer.invoke({\n",
    "    'question': query,\n",
    "    'context': context\n",
    "})\n",
    "\n",
    "# Run ranking\n",
    "ranking = rag_chain_rank.invoke({\n",
    "    'question': query,\n",
    "    'documents': documents\n",
    "})\n",
    "\n",
    "print(f'Final Answer: {answer}\\n')\n",
    "print(f'\\nRanked Document Indices: {ranking}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52e2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents:\n",
      "\n",
      "Doc 0: Green tea contains antioxidants that improve health.\n",
      "Doc 1: Green tea may improve metabolism and aid in weight loss.\n",
      "Doc 2: Black tea is rich in caffeine and boosts energy.\n"
     ]
    }
   ],
   "source": [
    "#  Display ranked documents\n",
    "\n",
    "# Ranking result is a string of indices, e.g. \"0, 2, 1\"\n",
    "# Parse into integers\n",
    "ranked_indices = [\n",
    "    int(i.strip())\n",
    "    for i in ranking.split(',') \n",
    "    if i.strip().isdigit()\n",
    "]\n",
    "\n",
    "print('Ranked Documents:\\n')\n",
    "\n",
    "#filter out any invalid indices in one step\n",
    "for idx in ranked_indices:\n",
    "    if idx < len(documents):\n",
    "        print(f'Doc {idx}: {documents[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9f7484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focused Answer (Top-Ranked Document Only):\n",
      "\n",
      "The context mentions that green tea improves health through its antioxidant content.\n"
     ]
    }
   ],
   "source": [
    "#  Focused answer using top-ranked document\n",
    "\n",
    "# Parse the ranking string into a list of valid indices\n",
    "ranked_indices = [\n",
    "    int(i.strip()) \n",
    "    for i in ranking.split(\",\") \n",
    "    if i.strip().isdigit() and int(i.strip()) < len(documents)\n",
    "]\n",
    "\n",
    "# If we have at least one ranked index, take the top one\n",
    "if ranked_indices:\n",
    "    top_idx = ranked_indices[0]\n",
    "    top_doc = documents[top_idx]\n",
    "\n",
    "    # Run the answer chain using only the top-ranked document\n",
    "    focused_answer = rag_chain_answer.invoke({\n",
    "        \"question\": query,\n",
    "        \"context\": top_doc\n",
    "    })\n",
    "\n",
    "    print(\"Focused Answer (Top-Ranked Document Only):\\n\")\n",
    "    print(focused_answer)\n",
    "else:\n",
    "    print(\"No valid ranked indices returned by the LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbe9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Broad Answer (All Retrieved Docs) ===\n",
      "\n",
      "Green tea contains antioxidants that improve health and may improve metabolism and aid in weight loss.\n",
      "\n",
      "=== Focused Answer (Top-Ranked Doc Only) ===\n",
      "\n",
      "The context mentions that green tea improves health due to its antioxidant content. However, specific health benefits are not provided in the context.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell: Compare broad vs focused answers\n",
    "# -----------------------------\n",
    "\n",
    "# Broad answer: uses all retrieved docs\n",
    "broad_answer = rag_chain_answer.invoke({\n",
    "    \"question\": query,\n",
    "    \"context\": context\n",
    "})\n",
    "\n",
    "# Focused answer: uses only the top-ranked doc\n",
    "ranked_indices = [\n",
    "    int(i.strip()) \n",
    "    for i in ranking.split(\",\") \n",
    "    if i.strip().isdigit() and int(i.strip()) < len(documents)\n",
    "]\n",
    "\n",
    "focused_answer = None\n",
    "if ranked_indices:\n",
    "    top_idx = ranked_indices[0]\n",
    "    top_doc = documents[top_idx]\n",
    "    focused_answer = rag_chain_answer.invoke({\n",
    "        \"question\": query,\n",
    "        \"context\": top_doc\n",
    "    })\n",
    "\n",
    "# Print side-by-side comparison\n",
    "print(\"=== Broad Answer (All Retrieved Docs) ===\\n\")\n",
    "print(broad_answer)\n",
    "\n",
    "print(\"\\n=== Focused Answer (Top-Ranked Doc Only) ===\\n\")\n",
    "print(focused_answer if focused_answer else \"No valid ranked indices returned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b398055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
