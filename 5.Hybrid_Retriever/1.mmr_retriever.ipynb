{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38426de0",
   "metadata": {},
   "source": [
    "MMR Retriever Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdd0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736850fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e8e9ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.\\nLangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.\\nFAISS: FAISS is a high-performance, open-source library developed by Metaâ€™s AI research team that specializes in the efficient similarity search and clustering of dense vectors at a massive scale, utilizing advanced indexing techniques like k-means clustering and product quantization to handle datasets containing billions of entries.\\nAgent: An AI Agent functions as a sophisticated reasoning entity that utilizes a large language model to dynamically determine which actions to take and which tools to use, effectively bridging the gap between static models and the real world by interacting with web searches, code execution, and databases.\\nLLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.\\nRAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.\\nMemory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.\\nBM25: BM25 is a classic and highly effective ranking function used by search engines to estimate document relevance based on keyword frequency and document length, excelling at exact keyword matching where semantic search might fail to find specific technical terms or IDs.\\nLangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.\\nLangGraph: By allowing agents to revisit previous steps, refine their decisions, and maintain a shared state across long-running sessions, LangGraph provides the cyclical logic necessary for building production-grade agentic systems where transparency and persistence are critical for success.\\nFAISS: By leveraging GPU acceleration to reduce latency, FAISS can execute nearest-neighbor searches in high-dimensional spaces with remarkable speed, acting as the primary backbone for many modern vector databases that require rapid information retrieval.\\nAgent: Unlike traditional sequential scripts, an agent operates in a continuous \"thought-action-observation\" loop, where it observes the output of its previous tools and adjusts its strategy in real-time to accomplish high-level goals provided by the user.\\nLLM: As the engine of modern generative AI, these models serve as general-purpose interfaces that can be fine-tuned or prompted for specific tasks ranging from coding assistantship to emotional sentiment analysis, all while demonstrating sophisticated logical reasoning.\\nRAG: The RAG process involves converting a user\\'s query into a vector embedding, searching a vector database for similar content, and then inserting that content into the final prompt to ensure the model has the necessary context to generate a factual response.\\nMemory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.\\nBM25: In the modern RAG stack, BM25 is often paired with vector-based retrieval in a hybrid search system to ensure that the AI captures both the literal terms used in a query and the broader conceptual intent of the user for maximum retrieval accuracy.\\nLangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.\\nLangGraph: The framework is particularly valuable for developers who need to visualize and debug the intricate decision-making paths of autonomous AI actors, as it facilitates the creation of complex loops and state management strategies that traditional sequential scripts simply cannot support.\\nFAISS: In the context of generative AI, this library enables systems to quickly retrieve the most relevant document embeddings to serve as context for language model prompts, ensuring that the retrieval process remains both scalable and highly accurate during real-time operations.\\nAgent: By acting as an autonomous actor capable of providing answers that go beyond its original training data, the agent provides a flexible interface for complex problem-solving where the model must decide the best path to reach a specific objective without pre-defined hardcoding.\\nLLM: By learning statistical patterns from vast datasets, LLMs can perform diverse functions such as translation and prediction, providing a powerful foundation for creative writing and complex data interpretation that scales across numerous professional domains.\\nRAG: By first retrieving relevant facts before allowing the model to generate a reply, RAG ensures that AI systems can access information not included in their initial training set, making them far more reliable for enterprise and research applications.\\nMemory: Without an effective memory system, it would be impossible to build coherent assistants that can follow complex, multi-day threads of conversation, as the module provides the persistent state required for truly interactive and personalized AI experiences.\\nBM25: By providing a statistically grounded way to rank the importance of words within a document, BM25 remains an essential component for high-precision search applications that require a balance between keyword density and the global importance of specific search terms.\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load \n",
    "loader = TextLoader('sample.txt')\n",
    "raw_doc = loader.load()\n",
    "raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e461888e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: FAISS is a high-performance, open-source library developed by Metaâ€™s AI research team that specializes in the efficient similarity search and clustering of dense vectors at a massive scale, utilizing advanced indexing techniques like k-means clustering and product quantization to handle datasets containing billions of entries.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: An AI Agent functions as a sophisticated reasoning entity that utilizes a large language model to dynamically determine which actions to take and which tools to use, effectively bridging the gap between static models and the real world by interacting with web searches, code execution, and databases.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='RAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: BM25 is a classic and highly effective ranking function used by search engines to estimate document relevance based on keyword frequency and document length, excelling at exact keyword matching where semantic search might fail to find specific technical terms or IDs.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: By allowing agents to revisit previous steps, refine their decisions, and maintain a shared state across long-running sessions, LangGraph provides the cyclical logic necessary for building production-grade agentic systems where transparency and persistence are critical for success.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: By leveraging GPU acceleration to reduce latency, FAISS can execute nearest-neighbor searches in high-dimensional spaces with remarkable speed, acting as the primary backbone for many modern vector databases that require rapid information retrieval.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: Unlike traditional sequential scripts, an agent operates in a continuous \"thought-action-observation\" loop, where it observes the output of its previous tools and adjusts its strategy in real-time to accomplish high-level goals provided by the user.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: As the engine of modern generative AI, these models serve as general-purpose interfaces that can be fine-tuned or prompted for specific tasks ranging from coding assistantship to emotional sentiment analysis, all while demonstrating sophisticated logical reasoning.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content=\"RAG: The RAG process involves converting a user's query into a vector embedding, searching a vector database for similar content, and then inserting that content into the final prompt to ensure the model has the necessary context to generate a factual response.\"),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: In the modern RAG stack, BM25 is often paired with vector-based retrieval in a hybrid search system to ensure that the AI captures both the literal terms used in a query and the broader conceptual intent of the user for maximum retrieval accuracy.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: The framework is particularly valuable for developers who need to visualize and debug the intricate decision-making paths of autonomous AI actors, as it facilitates the creation of complex loops and state management strategies that traditional sequential scripts simply cannot support.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: In the context of generative AI, this library enables systems to quickly retrieve the most relevant document embeddings to serve as context for language model prompts, ensuring that the retrieval process remains both scalable and highly accurate during real-time operations.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: By acting as an autonomous actor capable of providing answers that go beyond its original training data, the agent provides a flexible interface for complex problem-solving where the model must decide the best path to reach a specific objective without pre-defined hardcoding.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: By learning statistical patterns from vast datasets, LLMs can perform diverse functions such as translation and prediction, providing a powerful foundation for creative writing and complex data interpretation that scales across numerous professional domains.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='RAG: By first retrieving relevant facts before allowing the model to generate a reply, RAG ensures that AI systems can access information not included in their initial training set, making them far more reliable for enterprise and research applications.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: Without an effective memory system, it would be impossible to build coherent assistants that can follow complex, multi-day threads of conversation, as the module provides the persistent state required for truly interactive and personalized AI experiences.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: By providing a statistically grounded way to rank the importance of words within a document, BM25 remains an essential component for high-precision search applications that require a balance between keyword density and the global importance of specific search terms.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Chunk the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = splitter.split_documents(raw_doc)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ed7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd0d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x292e4bed150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: \n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "\n",
    "\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d12527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create MMR Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    serach_type='mmr',\n",
    "    search_kwargs={'k': 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b8a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prompt and LLM\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '''\n",
    "    Answer the question based on the context provided.\n",
    "    \n",
    "    Answer the question based on the context provided.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {input}\n",
    "    '''\n",
    ")\n",
    "\n",
    "llm = init_chat_model('groq:llama-3.3-70b-versatile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f67d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: RAG Pipeline\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm= llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever, combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190360f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "According to the context provided, LangChain supports agents and memory in the following ways:\n",
      "\n",
      "1. **Memory**: LangChain has a dedicated module for memory, which allows conversational AI to store and retrieve information from past interactions. This enables the AI to maintain context and remember user preferences over multiple turns.\n",
      "\n",
      "2. **Agents**: LangChain, through its library LangGraph, supports stateful, multi-agent applications. LangGraph introduces a graph-based architecture that represents workflows as a series of nodes and edges, providing granular control over execution. This allows for complex, non-linear interactions and decision-making processes, which can involve multiple agents.\n",
      "\n",
      "In summary, LangChain supports agents by providing a framework for multi-agent applications through LangGraph, and it supports memory by having a dedicated module for storing and retrieving information from past interactions.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query\n",
    "query = {'input': 'How does langchain support agents and memory?'}\n",
    "response = rag_chain.invoke(query)\n",
    "#response\n",
    "print(f\"Answer:\\n{response['answer']}\")\n",
    "#print('Answer:\\n', response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281b891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does langchain support agents and memory?',\n",
       " 'context': [Document(id='39d498e9-fcb1-475d-894e-739cb18e5b78', metadata={'source': 'sample.txt'}, page_content='Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.'),\n",
       "  Document(id='63f5170f-c858-4c68-944d-d04c14d03d8a', metadata={'source': 'sample.txt'}, page_content='LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.'),\n",
       "  Document(id='d29c9519-d2b7-4cc1-b792-8a29e5e8b36c', metadata={'source': 'sample.txt'}, page_content='LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.')],\n",
       " 'answer': 'According to the context provided, LangChain supports agents and memory in the following ways:\\n\\n1. **Memory**: LangChain has a dedicated module for memory, which allows conversational AI to store and retrieve information from past interactions. This enables the AI to maintain context and remember user preferences over multiple turns.\\n\\n2. **Agents**: LangChain, through its library LangGraph, supports stateful, multi-agent applications. LangGraph introduces a graph-based architecture that represents workflows as a series of nodes and edges, providing granular control over execution. This allows for complex, non-linear interactions and decision-making processes, which can involve multiple agents.\\n\\nIn summary, LangChain supports agents by providing a framework for multi-agent applications through LangGraph, and it supports memory by having a dedicated module for storing and retrieving information from past interactions.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04121872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
