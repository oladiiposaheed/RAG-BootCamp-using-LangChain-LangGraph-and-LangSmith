{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a7fd22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8c3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0c3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content='Langchain helps to build LLM applications.'),\n",
    "    Document(page_content=\"Lion is the king of wild animals.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"LLM is a field in law in art which has nothing to do with TECH and IT.\"),\n",
    "    Document(page_content=\"Langchain has different types of retrievers.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb65ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_17180\\2750873916.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Step 2:Dense Retriever using HuggigFace and FAISS\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "dense_vectorstore  =FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb2da07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E832BA5C10>, search_kwargs={'k': 3}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001E83493C090>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Sparse Retriever using BM25\n",
    "sparse_retiever = BM25Retriever.from_documents(docs)\n",
    "sparse_retiever.k=3 # Top - documents to retrieve\n",
    "\n",
    "# Step 4: Combine Dense and Sparse Retriever with Ensemble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retiever],\n",
    "    weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17004397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1:\n",
      "Langchain helps to build LLM applications.\n",
      "\n",
      " Document 2:\n",
      "LLM is a field in law in art which has nothing to do with TECH and IT.\n",
      "\n",
      " Document 3:\n",
      "Langchain can be used to develop agentic ai application.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and output\n",
    "query = 'How to design and build application using LLM?'\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f'\\n Document {i+1}:\\n{doc.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f098e8",
   "metadata": {},
   "source": [
    "RAG Pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c60203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f2283c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E836D65150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E838202990>, root_client=<openai.OpenAI object at 0x000001E8367CEED0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E838202710>, model_name='gpt-3.5-turbo-0125', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), request_timeout=300.0, max_retries=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Prompt Template\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '''Answer the question based on the context below.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {input}\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Step 7\n",
    "llm = init_chat_model(\n",
    "    'openai:gpt-3.5-turbo-0125',\n",
    "    temperature=0.2,\n",
    "    request_timeout=300,\n",
    "    max_retries=5\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a91311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E832BA5C10>, search_kwargs={'k': 3}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001E83493C090>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the question based on the context below.\\n\\n    Context: {context}\\n\\n    Question: {input}\\n    ')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E836D65150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E838202990>, root_client=<openai.OpenAI object at 0x000001E8367CEED0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E838202710>, model_name='gpt-3.5-turbo-0125', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), request_timeout=300.0, max_retries=5)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step8:\n",
    "# Create stuff document chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Crete full rag chain\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8549927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Langchain can be used to design and build applications using LLM by incorporating agentic AI technology.\n",
      "\n",
      "Sourse Documents:\n",
      "\n",
      "Doc 1: Langchain helps to build LLM applications.\n",
      "\n",
      "Doc 2: LLM is a field in law in art which has nothing to do with TECH and IT.\n",
      "\n",
      "Doc 3: Langchain can be used to develop agentic ai application.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask question\n",
    "\n",
    "query = {\n",
    "    'input': 'How to design and build application using LLM?'\n",
    "}\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"Answer:\\n{response['answer']}\")\n",
    "\n",
    "print('\\nSourse Documents:')\n",
    "\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f'\\nDoc {i+1}: {doc.page_content}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3210ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c64d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
