{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205702df",
   "metadata": {},
   "source": [
    "Reranking Hybrid Statergy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca928c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5234fbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.\\nLangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.\\nFAISS: FAISS is a high-performance, open-source library developed by Metaâ€™s AI research team that specializes in the efficient similarity search and clustering of dense vectors at a massive scale, utilizing advanced indexing techniques like k-means clustering and product quantization to handle datasets containing billions of entries.\\nAgent: An AI Agent functions as a sophisticated reasoning entity that utilizes a large language model to dynamically determine which actions to take and which tools to use, effectively bridging the gap between static models and the real world by interacting with web searches, code execution, and databases.\\nLLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.\\nRAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.\\nMemory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.\\nBM25: BM25 is a classic and highly effective ranking function used by search engines to estimate document relevance based on keyword frequency and document length, excelling at exact keyword matching where semantic search might fail to find specific technical terms or IDs.\\nLangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.\\nLangGraph: By allowing agents to revisit previous steps, refine their decisions, and maintain a shared state across long-running sessions, LangGraph provides the cyclical logic necessary for building production-grade agentic systems where transparency and persistence are critical for success.\\nFAISS: By leveraging GPU acceleration to reduce latency, FAISS can execute nearest-neighbor searches in high-dimensional spaces with remarkable speed, acting as the primary backbone for many modern vector databases that require rapid information retrieval.\\nAgent: Unlike traditional sequential scripts, an agent operates in a continuous \"thought-action-observation\" loop, where it observes the output of its previous tools and adjusts its strategy in real-time to accomplish high-level goals provided by the user.\\nLLM: As the engine of modern generative AI, these models serve as general-purpose interfaces that can be fine-tuned or prompted for specific tasks ranging from coding assistantship to emotional sentiment analysis, all while demonstrating sophisticated logical reasoning.\\nRAG: The RAG process involves converting a user\\'s query into a vector embedding, searching a vector database for similar content, and then inserting that content into the final prompt to ensure the model has the necessary context to generate a factual response.\\nMemory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.\\nBM25: In the modern RAG stack, BM25 is often paired with vector-based retrieval in a hybrid search system to ensure that the AI captures both the literal terms used in a query and the broader conceptual intent of the user for maximum retrieval accuracy.\\nLangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.\\nLangGraph: The framework is particularly valuable for developers who need to visualize and debug the intricate decision-making paths of autonomous AI actors, as it facilitates the creation of complex loops and state management strategies that traditional sequential scripts simply cannot support.\\nFAISS: In the context of generative AI, this library enables systems to quickly retrieve the most relevant document embeddings to serve as context for language model prompts, ensuring that the retrieval process remains both scalable and highly accurate during real-time operations.\\nAgent: By acting as an autonomous actor capable of providing answers that go beyond its original training data, the agent provides a flexible interface for complex problem-solving where the model must decide the best path to reach a specific objective without pre-defined hardcoding.\\nLLM: By learning statistical patterns from vast datasets, LLMs can perform diverse functions such as translation and prediction, providing a powerful foundation for creative writing and complex data interpretation that scales across numerous professional domains.\\nRAG: By first retrieving relevant facts before allowing the model to generate a reply, RAG ensures that AI systems can access information not included in their initial training set, making them far more reliable for enterprise and research applications.\\nMemory: Without an effective memory system, it would be impossible to build coherent assistants that can follow complex, multi-day threads of conversation, as the module provides the persistent state required for truly interactive and personalized AI experiences.\\nBM25: By providing a statistically grounded way to rank the importance of words within a document, BM25 remains an essential component for high-precision search applications that require a balance between keyword density and the global importance of specific search terms.\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file\n",
    "loader = TextLoader('sample.txt')\n",
    "raw_docs = loader.load()\n",
    "raw_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541de49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: FAISS is a high-performance, open-source library developed by Metaâ€™s AI research team that specializes in the efficient similarity search and clustering of dense vectors at a massive scale, utilizing advanced indexing techniques like k-means clustering and product quantization to handle datasets containing billions of entries.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: An AI Agent functions as a sophisticated reasoning entity that utilizes a large language model to dynamically determine which actions to take and which tools to use, effectively bridging the gap between static models and the real world by interacting with web searches, code execution, and databases.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='RAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: BM25 is a classic and highly effective ranking function used by search engines to estimate document relevance based on keyword frequency and document length, excelling at exact keyword matching where semantic search might fail to find specific technical terms or IDs.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: By allowing agents to revisit previous steps, refine their decisions, and maintain a shared state across long-running sessions, LangGraph provides the cyclical logic necessary for building production-grade agentic systems where transparency and persistence are critical for success.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: By leveraging GPU acceleration to reduce latency, FAISS can execute nearest-neighbor searches in high-dimensional spaces with remarkable speed, acting as the primary backbone for many modern vector databases that require rapid information retrieval.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: Unlike traditional sequential scripts, an agent operates in a continuous \"thought-action-observation\" loop, where it observes the output of its previous tools and adjusts its strategy in real-time to accomplish high-level goals provided by the user.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: As the engine of modern generative AI, these models serve as general-purpose interfaces that can be fine-tuned or prompted for specific tasks ranging from coding assistantship to emotional sentiment analysis, all while demonstrating sophisticated logical reasoning.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content=\"RAG: The RAG process involves converting a user's query into a vector embedding, searching a vector database for similar content, and then inserting that content into the final prompt to ensure the model has the necessary context to generate a factual response.\"),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: In the modern RAG stack, BM25 is often paired with vector-based retrieval in a hybrid search system to ensure that the AI captures both the literal terms used in a query and the broader conceptual intent of the user for maximum retrieval accuracy.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LangGraph: The framework is particularly valuable for developers who need to visualize and debug the intricate decision-making paths of autonomous AI actors, as it facilitates the creation of complex loops and state management strategies that traditional sequential scripts simply cannot support.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='FAISS: In the context of generative AI, this library enables systems to quickly retrieve the most relevant document embeddings to serve as context for language model prompts, ensuring that the retrieval process remains both scalable and highly accurate during real-time operations.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Agent: By acting as an autonomous actor capable of providing answers that go beyond its original training data, the agent provides a flexible interface for complex problem-solving where the model must decide the best path to reach a specific objective without pre-defined hardcoding.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='LLM: By learning statistical patterns from vast datasets, LLMs can perform diverse functions such as translation and prediction, providing a powerful foundation for creative writing and complex data interpretation that scales across numerous professional domains.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='RAG: By first retrieving relevant facts before allowing the model to generate a reply, RAG ensures that AI systems can access information not included in their initial training set, making them far more reliable for enterprise and research applications.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='Memory: Without an effective memory system, it would be impossible to build coherent assistants that can follow complex, multi-day threads of conversation, as the module provides the persistent state required for truly interactive and personalized AI experiences.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='BM25: By providing a statistically grounded way to rank the importance of words within a document, BM25 remains an essential component for high-precision search applications that require a balance between keyword density and the global importance of specific search terms.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e0e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User query\n",
    "query = 'How to use langchain to build memory application?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05695206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS anf HuggingFace model Embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cf5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa7e4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000185E683D850>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79638e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OPENAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7fdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# if api_key is None:\n",
    "#     raise ValueError('OPENAI_API_KEY is not set')\n",
    "\n",
    "# print(f'API Key loaded: {api_key[:5]} + .........')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai = OpenAIEmbeddings()\n",
    "vectorstore_openai = FAISS.from_documents(docs, embedding_openai)\n",
    "retriever_openai = vectorstore_openai.as_retriever(search_kwargs={'k': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2494591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000185ECA27D90>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfb005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d51392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000185ED773450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000185ECFC7010>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define prompt and use llm\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = init_chat_model('groq:llama-3.1-8b-instant')\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7027995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "\n",
    "# Define the template for how the LLM should process the documents\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. Your task is to rank the following documents from most to least relevant.\n",
    "    \n",
    "    User Question: '{question}'\n",
    "    \n",
    "    documents: {documents}\n",
    "        -Think about the relevance of each document to the user's question.\n",
    "        -Return a list of document indices in ranked order, starting from the most relevant.\n",
    "        \n",
    "    Output format: comma separated document indices (e.g. , 2, 3, 0,...)\n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff5dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template=\"\\n    You are a helpful assistant. Your task is to rank the following documents from most to least relevant.\\n\\n    User Question: '{question}'\\n\\n    documents: {documents}\\n        -Think about the relevance of each document to the user's question.\\n        -Return a list of document indices in ranked order, starting from the most relevant.\\n\\n    Output format: comma separated document indices (e.g. , 2, 3, 0,...)\\n\\n    \")\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000185ED773450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000185ECFC7010>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create chain\n",
    "\n",
    "# Build the LCEL chain (Prompt -> Model -> String Output)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265e5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1da02bab-954f-48a5-8bdc-80f158667d72', metadata={'source': 'sample.txt'}, page_content='Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.'),\n",
       " Document(id='1ba3abfd-29f4-4df6-bd0e-7052efc00a1f', metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.'),\n",
       " Document(id='b55cf1d9-3d8e-4f06-b0af-2b7eb34230fb', metadata={'source': 'sample.txt'}, page_content='LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.'),\n",
       " Document(id='be62dada-ec30-4ff5-99e9-6923658bb09e', metadata={'source': 'sample.txt'}, page_content='LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.'),\n",
       " Document(id='d37b3d69-2d68-4648-8cfd-2bb8cffb70af', metadata={'source': 'sample.txt'}, page_content='LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.'),\n",
       " Document(id='e262a466-d239-4657-917f-4b597e82316f', metadata={'source': 'sample.txt'}, page_content='Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.'),\n",
       " Document(id='9d172b48-0285-4a15-9e78-d2feb573caac', metadata={'source': 'sample.txt'}, page_content='RAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.'),\n",
       " Document(id='1c736b60-a898-4e35-b522-d2be78f0c03a', metadata={'source': 'sample.txt'}, page_content='LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch relevant documents using your existing retriever\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fd06a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.', '2. LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.', '3. LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.', '4. LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.', '5. LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.', '6. Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.', '7. RAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.', '8. LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.']\n",
      "\n",
      "1. Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.\n",
      "2. LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.\n",
      "3. LangChain: By centering its core philosophy on the \"Chain\" concept, the framework enables the automated linking of various componentsâ€”such as memory, external tools, and promptsâ€”into a unified workflow designed to handle multi-step reasoning tasks while building robust AI systems that maintain data connectivity across different environments.\n",
      "4. LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.\n",
      "5. LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.\n",
      "6. Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.\n",
      "7. RAG: Retrieval-Augmented Generation is an architectural pattern that enhances the accuracy of language models by grounding their output in specific retrieved documents from verified knowledge bases, which effectively reduces \"hallucinations\" and provides up-to-date information.\n",
      "8. LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.\n"
     ]
    }
   ],
   "source": [
    "# Create a numbered list of the document contents\n",
    "doc_lines = [\n",
    "    f'{i + 1}. {doc.page_content}' for i, doc in enumerate(retrieved_docs)\n",
    "]\n",
    "\n",
    "# Join the lines into a single string for the prompt\n",
    "formatted_docs = '\\n'.join(doc_lines)\n",
    "print(doc_lines)\n",
    "print('')\n",
    "print(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb1935dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranked Indices:\n",
      "Based on the user question 'How to use langchain to build memory application?', I would rank the documents as follows:\n",
      "\n",
      "3, 1, 4, 2, 5, 6, 8, 7\n",
      "\n",
      "Here's a brief explanation for each ranking:\n",
      "\n",
      "1. Document 3: This document directly mentions the concept of \"Chain\" in LangChain, which is closely related to building memory applications. It mentions the automated linking of various components, including memory, which is exactly what the user is asking for.\n",
      "\n",
      "2. Document 1: This document specifically explains the concept of Memory in LangChain, which is a key component in building memory applications. It provides a clear understanding of how LangChain's memory module works.\n",
      "\n",
      "3. Document 4: This document describes the modular architecture of LangChain, which enables developers to construct sophisticated AI pipelines. While it doesn't directly mention memory applications, it provides a general understanding of how LangChain works, which is relevant to the user's question.\n",
      "\n",
      "4. Document 2: This document provides a general overview of LangChain, its features, and its architecture. It's a good starting point for understanding LangChain, but it doesn't directly address the user's question about building memory applications.\n",
      "\n",
      "5. Document 5: This document introduces LangGraph, a specialized library within the LangChain ecosystem, but it's not directly related to building memory applications.\n",
      "\n",
      "6. Document 6: This document mentions various specialized memory types in LangChain, but it doesn't provide a clear explanation of how to use them to build memory applications.\n",
      "\n",
      "7. Document 8: This document explains the concept of LLM, but it's not directly related to building memory applications in LangChain.\n",
      "\n",
      "8. Document 7: This document introduces the RAG architectural pattern, but it's not directly related to building memory applications in LangChain.\n",
      "\n",
      "By ranking the documents in this order, you'll get a good understanding of how to use LangChain to build memory applications, from the specific details of Memory in LangChain to the general architecture and features of the framework.\n"
     ]
    }
   ],
   "source": [
    "# Reranking\n",
    "#Invoke the chain with the variables\n",
    "response = chain.invoke({\n",
    "    'question': query,\n",
    "    'documents': formatted_docs,\n",
    "})\n",
    "\n",
    "# 3. View the final ranked list\n",
    "print(\"Re-ranked Indices:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1cf0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 1, 4, 5, 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = []                          # Empty list\n",
    "\n",
    "for i in response.split(','):         # Loop over items split by comma\n",
    "    cleaned = i.strip()               # Remove spaces\n",
    "    if cleaned.isdigit():             # Check if digits only\n",
    "        number = int(cleaned) - 1     # Convert to int, subtract 1\n",
    "        indices.append(number)        # Add to list\n",
    "\n",
    "indices                               # Show result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa23168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1da02bab-954f-48a5-8bdc-80f158667d72', metadata={'source': 'sample.txt'}, page_content='Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.'),\n",
       " Document(id='be62dada-ec30-4ff5-99e9-6923658bb09e', metadata={'source': 'sample.txt'}, page_content='LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.'),\n",
       " Document(id='1ba3abfd-29f4-4df6-bd0e-7052efc00a1f', metadata={'source': 'sample.txt'}, page_content='LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.'),\n",
       " Document(id='d37b3d69-2d68-4648-8cfd-2bb8cffb70af', metadata={'source': 'sample.txt'}, page_content='LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.'),\n",
       " Document(id='e262a466-d239-4657-917f-4b597e82316f', metadata={'source': 'sample.txt'}, page_content='Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.'),\n",
       " Document(id='1c736b60-a898-4e35-b522-d2be78f0c03a', metadata={'source': 'sample.txt'}, page_content='LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse and rerank\n",
    "\n",
    "# indices = [int(i.strip()) - 1 for x in response.split(',') if i.strip().isdigit()]\n",
    "# indices\n",
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed97a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1:\n",
      "Memory: Memory in LangChain refers to the dedicated module responsible for storing and retrieving information from past interactions, which allows conversational AI to maintain context and remember user preferences over multiple turns rather than treating each prompt as an isolated event.\n",
      "\n",
      "Rank 2:\n",
      "LangChain: This modular architecture empowers developers to construct sophisticated AI pipelines where various document loaders and sequence chains work in harmony, ensuring that the transition between diverse language models remains fluid and the overall system stays resilient to changes in the underlying technology stack.\n",
      "\n",
      "Rank 3:\n",
      "LangChain: LangChain serves as an extensive open-source orchestration framework that simplifies the creation of complex, LLM-powered applications by providing a modular suite of tools for prompt management and document loading while abstracting the intricacies of model integration to allow for seamless switching between different APIs with minimal code changes.\n",
      "\n",
      "Rank 4:\n",
      "LangGraph: LangGraph is a specialized library within the LangChain ecosystem that introduces a robust, graph-based architecture to enable stateful, multi-agent applications that move beyond linear paths by representing workflows as a series of nodes and edges for granular control over execution.\n",
      "\n",
      "Rank 5:\n",
      "Memory: The framework offers various specialized memory types, such as `ConversationBufferMemory` for raw chat history and `ConversationSummaryMemory` for token-efficient summaries, ensuring that the assistant can handle long-running dialogues without losing track of the core topic.\n",
      "\n",
      "Rank 6:\n",
      "LLM: A Large Language Model is a deep learning algorithm trained on astronomical amounts of text data to recognize, summarize, and generate human-like content, utilizing the transformer architecture to process entire sequences of words simultaneously to capture deep context and nuance.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f'\\nRank {i}:\\n{doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427db9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
